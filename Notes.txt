ML Notes

Data Cleaning
1.  Change content to lower case
2.  Remove stop words such as " a , and " etc - reduces complexity and smaller dataset  and easier during training
3. Regex expression gotcha
    3.1. Use raw strings if your data has special chars that could be interpreted as something else Eg : "C:\desktop\notes"
         can be interpreted as string with new line "\n". but can be prevented using r"C:\desktop\notes"
4. Tokenization: Word Tokeniztion, sentence tokenization, char Tokenization, sub sentence tokenization (from nltk.tokenize import word_tokenize, sent_tokenize)
5. Stemming: where words are reduced to their base form. For example, words like ‘connecting’ or ‘connected’ will be stemmed to the base form ‘connect’. (from nltk.stem import PorterStemmer).
    Sometimes the base forms are not at all a words
6. Lemmatization: Where stemming removes the last few characters of a word, lemmatization stems the word to a more meaningful base form and ensures it does not lose it's meaning. 
    Lemmatization works more intelligently, referencing a pre-defined dictionary containing the context of words and uses this when diminishing the word to the base form. 


7. Types of Tagging( we are using NLK tool models such as spaCy to do both POS and NER )

1. Part of speech Tagging ( grouping  based on Noun, Pronoun, AJ)
2. Names Entity Recogniation (Place ,Date, Location, Person)


Sentement Analysis
1. Rules based ( struggles to understand sarcasm or irony)
2. Transformer based ( Basemodel, specific languaru)


Text Vectorization s the process of converting unstructured text into numerical representations (vectors) that machine learning models can process.
Since algorithms can’t directly work with raw text, vectorization provides a way to encode meaning, structure, or statistical properties of text in a mathematical format.
0.1  Count-Based Methods
    1. The bag of words
        1.1 Represents text as a vector of word counts (or frequencies).
        1.2 Ignores grammar and word order. Example: “dog bites man” and “man bites dog” will produce the same vector.
        1.3 Useful for simple text classification but limited in capturing semantics.

    2.TF-IDF (Term Frequency–Inverse Document Frequency):
        2.1 Weights words by how frequently they appear in a document vs. how unique they are across the corpus.
        2.2 Helps reduce the influence of very common words (like “the”, “is”) while highlighting distinctive terms.
0.2 Word Embeddings
0.3. Contextual Embeddings (Deep Learning Based)
0.4. Sentence & Document Embeddings
0.5. Hybrid / Advanced Methods

Summary
BoW / TF-IDF → Simple, interpretable, but limited.
Word2Vec / GloVe / FastText → Dense, semantic-rich, but context-independent.
ELMo / BERT family → Context-aware, powerful, state-of-the-art.
Sentence/Doc Embeddings → Work at higher granularity (sentences, paragraphs, documents).